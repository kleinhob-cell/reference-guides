# ğŸ§  AI Coding Models â€” VS Code + Continue + Ollama

## ğŸ“„ Overview
This guide documents the AI models installed via  
`/home/brent/ai-assistant-project/docs/ollama_model_setup.sh`  
and configured in Continue (v1.2.2) for coding, reviews, and meeting assistant workflows.

It covers:
- **Scope:** Model roles in the workflow
- **Hardware fit:** What runs well on RX 7900 XTX (24 GB)
- **Config mapping:** Continue model roles
- **Usage tips:** When to switch models

---

## ğŸ–¥ï¸ Hardware summary
- **CPU:** AMD Ryzen 9700X â€” high thread count for preprocessing/context ingestion
- **GPU:** AMD Radeon RX 7900 XTX (24 GB VRAM) â€” runs 13B dense and Mixtral comfortably, 70B quantized on-demand
- **Storage:** Samsung 980 PRO NVMe SSDs â€” fast model load times
- **OS:** Ubuntu 24.04 LTS â€” ROCm-friendly for Ollama

## ğŸ“Š Model Roles

| Role | Ollama Model Tag | Purpose |
|------|------------------|---------|
| **Autocomplete (fast)** | `qwen2.5-coder:1.5b` | Very fast, low VRAM, warningâ€‘free completions |
| **Autocomplete (richer)** | `starcoder2:3b` | Larger, still fast, trained for code completion |
| **Autocomplete/Chat hybrid** | `deepseek-coder:6.7b-base` | Higher quality completions, more VRAM |
| **Chat (default)** | `codellama:7b-instruct` | Quick code help, explanations |
| **Chat (deep coding)** | `codellama:13b-instruct` | Complex refactors, multiâ€‘file reasoning |
| **Review** | `mixtral:8x7b` | Senior Architect + Security Audit reviews |
| **Summarization/Creative** | `llama3`, `mistral`, `phi3`, `qwen2.5:14b`, `gemma:2b`, `gemma:7b`, `gemma2:9b`, `nous-hermes2:7b` | Meeting assistant, doc drafting, creative tasks |
| **Deep Reasoning (onâ€‘demand)** | `llama3:70b` | Very deep reasoning, large context â€” slower, high VRAM use |

## ğŸ“ˆ Hardware Fit Table

| Model | VRAM (Q4_K_M approx) | Fits in 24â€¯GB VRAM? | Notes |
|-------|----------------------|--------------------|-------|
| `qwen2.5-coder:1.5b` | ~2â€¯GB | âœ… | Instant load, ideal for autocomplete |
| `starcoder2:3b` | ~4â€“5â€¯GB | âœ… | Richer completions, still light |
| `deepseek-coder:6.7b-base` | ~7â€“8â€¯GB | âœ… | Higherâ€‘quality completions, more VRAM |
| `codellama:7b-instruct` | ~5â€“6â€¯GB | âœ… | Fast, responsive |
| `codellama:13b-instruct` | ~10â€“12â€¯GB | âœ… | Leaves headroom for context |
| `mixtral:8x7b` | ~12â€“14â€¯GB active | âœ… | Long context, multiâ€‘file reviews |
| `llama3` / `mistral` / `phi3` | ~4â€“8â€¯GB | âœ… | General reasoning, summarization |
| `qwen2.5:14b` | ~14â€“16â€¯GB | âœ… | Multilingual precision |
| `gemma` family | ~2â€“8â€¯GB | âœ… | Creative, docâ€‘friendly |
| `llama3:70b` | ~20â€“22â€¯GB | âš ï¸ | Fits, but minimal VRAM left â€” slower if spilling to RAM |

## ğŸ›  Continue Config Role Mapping

~~~bash
models:
  - name: qwen2.5coder1b
    provider: ollama
    model: qwen2.5-coder:1.5b
    roles: [autocomplete]

  - name: starcoder2_3b
    provider: ollama
    model: starcoder2:3b
    roles: [autocomplete]

  - name: deepseekcoder6.7b
    provider: ollama
    model: deepseek-coder:6.7b-base
    roles: [autocomplete, chat]

  - name: codellama7b
    provider: ollama
    model: codellama:7b-instruct
    roles: [chat]

  - name: codellama13b
    provider: ollama
    model: codellama:13b-instruct
    roles: [chat]

  - name: mixtral8x7b
    provider: ollama
    model: mixtral:8x7b
    roles: [chat]
~~~

## ğŸ’¡ Usage Tips

- **Dayâ€‘toâ€‘day coding:** Use `codellama:7b-instruct` for quick answers, small code snippets, and explanations.
- **Complex refactors:** Switch to `codellama:13b-instruct` when you need deeper reasoning or multiâ€‘file context.
- **Full project review:** Use `mixtral:8x7b` with the â€œArchitect Reviewâ€ or â€œSecurity Auditâ€ prompts for longâ€‘context, structured feedback.
- **Autocomplete testing:** Compare `qwen2.5-coder:1.5b` (fastest), `starcoder2:3b` (richer completions), and `deepseek-coder:6.7b-base` (highest quality) to see which feels best in your workflow.
- **Meeting assistant / creative work:** Switch to summarization/creative models (`llama3`, `mistral`, `phi3`, `qwen2.5:14b`, `gemma` family, `nous-hermes2:7b`) for drafting, summarizing, or brainstorming.
- **Deep reasoning:** Only load `llama3:70b` when you truly need maximum reasoning depth â€” it will consume most of your VRAM and run slower.

---

